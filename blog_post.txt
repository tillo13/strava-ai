Title: *Delving Deep into Strava Workouts with Python and Machine Learning*

*Intro:* Over the course of one weekend, I embarked on an insightful journey, plunging into my repository of ~3000 workouts on Strava with a clear goal - to study, explore, and unlock insights with the power of Machine Learning (ML). This blog will narrate my explorations, shedding light on how different stages of machine learning can be harnessed to extract value from raw data. I performed data extraction, processing, exploration, modeling, evaluation, and ultimately prediction, leveraging Python as my language of instruction and my code is available on GitHub for tech enthusiasts to delve deeper.

*(1) The First Milestone: Channeling Strava Data Into Our System - An Enhanced Walking Through*

Accumulating the raw Strava data was a challenge that I was prepared to take on. What procedure was adopted to accomplish this? Very simple. I utilized the power of Python and some of its libraries to extract data from the Strava API and then stored it in a proper format, allowing easy data management and further processing. Let's delve a bit deeper into the mechanics of this.

To begin with, the Python requests library was instrumental in connecting to the Strava API and retrieving my activity data. The access token, which authenticates our application and allows the API to recognize who we are, was first acquired through the check_token() function from initialize.py.

Here's a more extensive look at initialize.py, the script that makes the first few strides towards our data extraction journey.

python
...
# Get the latest access token
access_token = check_token()

# Setup the API call
url = "https://www.strava.com/api/v3/athlete/activities"
headers = {'Authorization': f'Bearer {access_token}'}
params = {'per_page': 7, 'page': 1}

# Send the GET request
response = requests.get(url, headers=headers, params=params)

...

Going forward, the last_n_activities.py script deals with the task of obtaining the Strava activity data. In this script, we use the requests.get() method and supply it with the API endpoint URL, our headers (which includes the access token), and additional parameters that specify the number of activities per page and the page number.

The response from the API call is a JSON object, which we then decode using response.json(). If the API response can be successfully decoded, we continue by saving it into individual JSON files.

python
...
# Get the JSON response
activities = response.json()

# Create a new directory to store our files
folder_path = './activities'
os.makedirs(folder_path, exist_ok=True)
existing_files = os.listdir(folder_path)

# Save each activity data into separate JSON files
num_duplicates = 0
num_new_activities = 0
...
for activity in activities:
    activity_id = activity['id']
    file_path = os.path.join(folder_path, f'{activity_id}.json')

    if f'{activity_id}.json' in existing_files:
        num_duplicates += 1
        continue
    
    with open(file_path, 'w') as file:
        json.dump(activity, file, indent=4)
    num_new_activities += 1

...

The script creates a dedicated folder named 'activities' to store each JSON file. To ensure the folder does not get flooded with duplicate files, we perform a simple check if the current activity's JSON file already exists in the folder directory before proceeding to write the file. Thus, this Python magic accomplishes the major step of localizing my raw data for the analysis stages that follow.

In sum, this mechanistic process beautifully accomplishes the first major milestone of our fitness data exploration, laying the groundwork for the exciting explorations that follow. Harnessing the power of Python and the Strava API, we successfully pull the activity data into our local system, ready and eager for further dissection and analysis. We efficiently circumnavigate potential roadblocks of data duplication and unintentional overwriting, effectively ensuring that our original data remains uncompromised, while always ready for extraction and analysis. Let's dive deeper into our next steps!

*(2) Sorting the Raw Data: Field Frequency and Missing Fields*

After pulling my raw Strava data, the focus of this stage was to sort and organize the data. More precisely, I was interested in determining the frequency of occurrence of different fields or data points across the 3000 Strava workouts represented as JSON files. 

python
folder = './data_cleansing/activities'  
files = glob.glob(os.path.join(folder, '*.json'))

def validate_files(files):
    field_counts = defaultdict(lambda: defaultdict(int))
    file_count = len(files)

Upon defining the file location and initializing a dictionary (field_counts) to keep track of field appearances, each JSON file is processed. Building a tally of each field's presence across the data sets:

python
for file in files:
    with open(file, 'r') as f:
        json_data = json.load(f)
        for field in json_data.keys():
            field_counts[field]['count'] += 1

After all files are processed, the script provides a report, including the total file count:

python
Number of files processed: 3000

Followed by the frequency of each field's appearance:

python
Frequency of Field Appearance:
'resource_state': Exists in 3000 files (100.00%)
'athlete': Exists in 3000 files (100.00%)
...

This exercise proves beneficial in understanding the distribution of data available across the different fields, such as 'resource_state', 'athlete', and many others that were appearing in 100% of files.

The script also identifies and lists fields that do not appear in every file, illuminating the possibility of missing data:

python
Fields missing in some files:
'workout_type': Missing in 1007 files (33.57%)
'average_cadence': Missing in 1012 files (33.73%)
...
'average_temp': Missing in 2999 files (99.97%)

This part of the journey is about data exploration. It gives a clear overview not only of the composition of the dataset but also of the quality of the data based on missing fields. This forms the groundwork for the subsequent process of further segregating the data and applying machine learning models on it.

*(3) Diving Deeper: Classifying Data by Platforms*

The uniqueness of certain workout events lies in the platform used to record each activity. This stage of data exploration drills down into exercise data classification based on the recording platform. For this purpose, a Python script named strava_activity_classification.py forms the backbone of the process. 

python
import os
import json

folder = './data_cleansing/activities'
files = os.listdir(folder)

In the code block above, the Python os module is used to inspect the directory structure, and the json module is used to handle JSON data. It opens the directory containing the workout files and lists all the files within this directory.

python
peloton_activities = {'Ride': 0, 'Yoga': 0, 'EBikeRide': 0, 'Run': 0, 'Walk': 0, 'Workout': 0, 'Other': 0}

Here, I initialized a dictionary to classify the types of Peloton activities represented in my data. Each key represents an activity type, and the value corresponding to each key is initialized to zero.

python
for filename in files:
    if filename.endswith('.json'):
        with open(os.path.join(folder, filename), 'r') as file:
            data = json.load(file)
Each file in our directory is opened and read. If the file is a JSON file, it is opened, and the JSON data is loaded into the data variable.

python
external_id = data.get('external_id')
trainer = data.get('trainer', False)
activity_type = data.get('type')
sport_type = data.get('sport_type')
These lines extract different elements from the JSON data, such as the external_id, whether a trainer was involved, the type of activity (activity_type), and the type of sport (sport_type). 

python
if external_id is not None and external_id.endswith('.tcx') and len(external_id) == 36:
    if trainer and not start_latlng and not end_latlng:
        if activity_type in peloton_activities.keys() and sport_type == activity_type:
            peloton_activities[activity_type] += 1
        else: 
            peloton_activities['Other'] += 1
        continue
In this segment, if the external_id of the file is not null and satisfies certain conditions - including ending with '.tcx' and having a length equal to 36 - we consider it a Peloton activity. If the sport_type matches the activity_type, we increment the activity count by one. If the activity type does not match any types known (i.e., specified in peloton_activities.keys()), we attribute it as an 'Other' type of workout.

There are similar blocks of code that focus on differentiating data from platforms such as ELEMNT BOLT, Zwift, Garmin Ping, and Strava. In the end, the script prints out a statistical summary of each type of workout for each platform, giving a clear picture of the proportion of my workouts being performed on various platforms. 

To sum up this section, the intricate scanning and sorting of activities completed on various platforms are part of the enlightening data exploration journey. This step provides an understanding of the preferred platforms based on the activity type, and this discovery might open doors to more focused studies, analysis, and possibly more detailed workout plans for fitness enthusiasts.

*(4) Highlighting the Heroes: Top Rides*

One particularly thrilling adventure was the design and deployment of a specialized scoring system for my cycling endeavors on the Peloton platform. Essentially, it was treating an accomplishment in cycling as a numerical triumph. This drew inspiration from the dynamic scoring mechanism coined as the 'Briskr score', which I expanded upon using Python magic. 

If you are interested in understanding more about the mechanics of developing a Briskr score, you can sneak a peek at this link: Boomi aids in staying healthy.

Implementing this system involved creating a function named calculate_score(data), which scores a ride solely on the power output indicated by the kilojoules data field. This evidence-based metric signifies one's exertion level in a ride and acts as an interesting score benchmark:

python
def calculate_score(data):
    return data['kilojoules']

To compartmentalize my rides based on their duration, I've rolled out an intelligent categorization function that extracts the ride duration from the activity name. It then checks if the expected duration is in sync with the actual one, accommodating a minor error tolerance of 2 minutes:

python
def categorize_ride(data):
    duration = data['elapsed_time'] / 60  # duration in minutes
    expected_duration = extract_time_from_name(data['name'])
    # the ride will be categorized only if name duration and actual duration are almost the same (with some error tolerance, you can change it)
    if expected_duration and abs(duration - expected_duration) <= 2:
        return expected_duration
    return None

I then cycle through each activity in the dataset, applying the scoring and categorization procedures. Activities that conform to specific conditions are considered Peloton rides, and these get categorized and scored:

python
for filename in files:
    if filename.endswith('.json'):
        filepath = os.path.join(directory, filename)
        ...
        score = calculate_score(data)
        peloton_ride_scores.setdefault(ride_category, {})[filename] = score
        ...

This way, each Peloton ride gets a score, and the ride is inserted into a dictionary under its due category, spawning a neatly segregated scoring system.

Post this segregation, I've maintained a specific category order for visually pleasing console outputs:

python
categories_order = [5, 10, 15, 20, 30, 45, 60, 75, 90] 
sorted_peloton_ride_scores = {k: peloton_ride_scores[k] for k in categories_order if k in peloton_ride_scores}

In terms of presenting these findings, I've employed vibrant coloring, varying for different categories, enhancing the readability of the top rides for each category:

python
colors = [Fore.GREEN, Fore.YELLOW, Fore.BLUE, Fore.MAGENTA, Fore.RED, Fore.CYAN, Fore.LIGHTRED_EX, Fore.LIGHTGREEN_EX, Fore.LIGHTYELLOW_EX]

Upon structuring a table format with fields like ride timing, the computed score, average speed, distance covered, and watts, we get an insight-rich console output that aggrandizes the ride information with rich multi-colored detailing:

python
print('\n' + colors[idx] + f'Top rides for {category} mins:' + Style.RESET_ALL)
headers = ["Mins", "Ride Date", "kJ Score", "Avg Speed(mph)", "Distance(miles)", "Avg Watts", "kJ/min", "Kudos Count", "Filename", "Ride Name"] 
print(tabulate(table, headers=headers))

All in all, the charted journey through my cycling activities was an exploratory adventure in itself, with a touch of personalized scoring that was not only informative but also continuously evoked a sense of exploration, inspiring me and hopefully inspiring others to keep pedaling harder.

**(5) Timing Meticulously: Classification of Workouts**

After a comprehensive gathering and classification of workout data, the next significant endeavor was to shine a spotlight on the chronology of fitness routines – pinpointing not just what makes up a fitness regimen but *when* these routines typically occur in the flow of one's daily life. With Python and some clever programming maneuvers, I turned my attention to the temporal dimension of my workouts. Below, we'll take a voyage through time – narrating how our workout habits ebb and flow in harmony with the rhythm of our everyday schedules.

The Python script written to segment workouts according to their respective scheduling was titled `when_you_workout.py`. Focused on sorting workouts by the hour of the day, my aim was to render a spectrum of workout frequencies distributed across the 24-hour period of a day.

```python
import os
import json
from datetime import datetime
from collections import defaultdict
from pytz import timezone
from tabulate import tabulate
from termcolor import colored
...
for filename in files:
    with open(os.path.join(folder, filename), 'r') as file:
        data = json.load(file)
...
        custom_time = start_datetime.astimezone(custom_tz)
        workouts_per_hour[custom_time.hour] += 1
...

This block of code fetches JSON-formatted workout activities from a directory named 'activities'. For each JSON file, using the Python datetime function, the program reads the exact timestamp of when a workout was started (start_date_local). 

The time is then converted into a custom timezone (America/Los_Angeles as an example) using the pytz Python library. The hour component of the timestamp is extracted and used as a key to increment the count of workouts for that particular hour in the workouts_per_hour dictionary. This dictionary will ultimately hold the grand tally of workouts conducted across each hour of the day.

Subsequently, the script also retrieves other interesting attributes related to a workout, such as 'max_heartrate', 'average_heartrate', 'kilojoules', 'weighted_average_watts', 'max_watts', 'average_watts', 'average_cadence', 'max_speed', 'average_speed', 'elapsed_time', 'distance':

python
attrs = ['max_heartrate', 'average_heartrate', 'kilojoules', 'weighted_average_watts', 'max_watts', 'average_watts', 'average_cadence', 'max_speed', 'average_speed', 'elapsed_time', 'distance']
attr_values = defaultdict(lambda: defaultdict(list))
...
for attr in attrs:
    value = data.get(attr)
    if value is not None and value != 0:
        if attr == 'elapsed_time':
            value /= 60
        elif attr == 'distance':
            value /= 1609.34
        elif 'speed' in attr:
            value *= 2.23694  # Convert to miles per hour
        attr_values[custom_time.hour][attr].append(value)
...

For a given hour, this block of code fetches the respective attribute values from the JSON file and stores them in a nested dictionary attr_values. This dictionary will store each of these attributes against the respective workout hour, laying the foundation for eventually computing the average statistical value for each attribute during a particular hour of the day.

Building onward, the script organizes all the collected hourly-averaged attributes and workout count data in a tabulated manner using the Python tabulate function. This function simplifies the construction of aesthetically pleasing ASCII tables:

python
table_data = []
...
for hour in range(24):  
    if hour in workouts_per_hour:
        count = workouts_per_hour[hour]
        averages = {attr: round(statistics.mean(remove_outliers(attr_values[hour][attr])), 2) for attr in attrs if attr_values[hour][attr]}
        row = [hour, count] + [averages.get(attr, 'N/A') for attr in attrs]
    else:
        row = [hour, 0] + ['N/A' for _ in attrs]
    table_data.append(row)

table_data = color_rows(table_data, attrs)
print(tabulate(table_data, headers=['Hour', 'Workouts'] + attrs))

This block of code displays the workout counts and hourly attribute averages in a structured, presentable tabular format. The table headers include workout hours ('Hour'), the number of workouts ('Workouts'), and other attributes related to the workouts. Each row in the table presents the corresponding data for each hour of the day. For empty data points, N/A is used to denote the absence of recorded workouts.

*(6) Leaping Forward: Kickstarting Machine Learning*

After meticulously cleaning and organizing the data, the next significant venture was to leverage it to offer predictions, assessments, and enlightening insights. Here is where Machine Learning makes a grand entrance. Using Machine Learning algorithms, we could extrapolate existing workout data, diving into captivating explorations with potential revelations.

This exploration started with constructing regression models that map relationships between several workout parameters and their effect on heart rate. Two regression models were created for this purpose - Linear Regression and Random Forest Regressor. 

Linear regression is a basic regression model which predicts a dependent variable value (y) based on a given independent variable (x). Here, it helped understand how different factors collectively influence the heart rate during workouts.

python
print('Linear Regression Results:')
model = LinearRegression()
model.fit(features_train, labels_train)

On the other hand, a Random Forest Regressor creates a set of decision trees from a randomly selected subset of the training set. It then aggregates the votes from different decision trees to decide the final prediction result. It deals excellently with nonlinear and high-dimension data, hence making it appropriate in this situation.

python
print('\nRandom Forest Regressor Results:')
model = RandomForestRegressor(n_estimators=100)
model.fit(features_train, labels_train)

The construction of these models was based on a dataset extracted from .json files in the 'activities' directory. This dataset, encapsulating an array of workout metrics like speed, cadence, watts, moving time, total elevation gain, kudos count, and more, scripted a comprehensive understanding of the workouts. 

python
...
for filename in files:
    if filename.endswith('.json'):
        with open(os.path.join(folder, filename), 'r') as file:
            ...
                
                if not any(v is None for v in data_dict.values()):
                    data_list.append(data_dict)
...

The above code block reads JSON files in the directory and collects relevant data into a list. After all files are processed, if the data list is not empty, it is converted into a Pandas DataFrame. 

From this dataframe, we drop any rows with missing values to ensure the most accurate training for our models. 

python
...
if not data_list:
    raise ValueError("No data found in the provided files.")

dataset = pd.DataFrame(data_list)

# Dropping any rows with missing values
dataset = dataset.dropna()
...

The 'average_heartrate' column serves as our labels (or the dependent variable), and the rest of the data as features (or the independent variables). This data is then used to split our dataset into a training set and a test set, with the test size being 20% of the total data. 

python
features = dataset[['average_speed', 'max_speed', 'average_cadence', 'average_watts', 'kilojoules', 'moving_time', 'total_elevation_gain', 'kudos_count', 'max_watts', 'weighted_average_watts', 'suffer_score']]
labels = dataset['average_heartrate']

features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.2)

After training each model, the prediction is executed, and their performance is evaluated using the Mean Squared Error (MSE) and R-squared (coefficient determination) metrics. 

python
...
labels_pred = model.predict(features_test)

print(colored(f'R-squared: {r2_score(labels_test, labels_pred)}', 'green'))
print(colored(f'MSE: {mean_squared_error(labels_test, labels_pred)}', 'blue'))

The process was first applied to the Linear Regression model and then the Random Forest Regressor. This resulted in an R-squared value of 0.508 for Linear Regression and a more impressive 0.774 for the Random Forest Regressor.

Linear Regression Results:
R-squared: 0.508
MSE: 150.39

Random Forest Regressor Results:
R-squared: 0.774
MSE: 68.83

The results were promising, showing that it was possible to predict an individual's average heart rate during a workout session based on various other factors. In the same stride, these findings suggest that the wealth of fitness data harbored a lot more knowledge yet to be unlocked. This realization broadens the path to more intricate data modeling and detailed analyses.

*Conclusion:* This journey served to underscore the immense possibilities that lie at the intersection of fitness and Machine Learning. The various stages, right from data extraction and cleaning to data modeling and evaluation, provided a firsthand appreciation of how machine learning algorithms can be employed to filter patterns and insights from data, mimic human learning patterns, and even make predictions.

The results from the Linear Regression Model and Random Forest Regressor were particularly enlightening. They demonstrated the feasibility of predicting an individual's heart rate during the course of a workout based on other workout factors. On a technical level, I found Random Forests to result in a better fit with substantially reduced error, affirming their robustness against overfitting compared to Linear Regression.

The project is not without its limitations and scopes for further refinement. It will be crucial to delve deeper into the data, identify outliers, and tackle skewness in data distribution for more accurate predictions. Moreover, exploring different model architectures and tuning hyperparameters can lead to improvements in prediction accuracy.

Ultimately, what started as a weekend coding project fostered an accelerated journey of exploration, learning, and inference. The meeting point of fitness and ML is indeed ripe with potential waiting to be tapped, and this venture has only scratched the tip of the iceberg.
